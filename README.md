<h1>Journey for my 66 Days for NLP </h1>

    Currently at the start point for learning Natural Language Processing. With this, i will
    set my path for achieving a small degree of knowledge in NLP and Data Science.  

    Paticularly following a friend's path #Thinam 


<strong>Day 1 of 66DaysOfNLP</strong>

<ul>
<l1>
    NLP or Natural Language Processing can simply be regarded as a field in Artificial Intelligence
    that focuses on the processing and indepth analyzing of Natural Language Data that sums up conversations between Human to Computers as well as Human to Human. 

In the following snapshot, I have displayed today's work progress that showcases trimming of certain text
by using a normal Python Library i.e. Regex. It is a simple program that removes Hashtag expressions, Retweet Text and hyperlinks from the input text and prints it out. 

</li>
</ul>


![](images/d1.PNG)

![](images/d1o.PNG)



<strong>Day 2 of 66DaysOfNLP</strong>

<ul>
<l1>
I would consider String Tokenization as one of the most crucial part while working with traditional NLP models or Advanced Deep Learning Models. The basic concept is to dissect a text or a string into tokens in the form of words or characters or sub words. With this we also have to remove stop words and punctuations as it doesn't add much value for the text.

In today's snapshot I have demonstrated the working of a Word Tokenizer which is one of the most commonly used tokenizer among Character and Sub Word Tokenizer. However, Words Tokenizer will suffer some problems while working with unknown tokens or word as compared to other types. I have implemented the Tokenizer to generate a list of clean word tokens or tweets from a given text string. 

</li>
</ul>


![](images/d2.PNG)

![](images/d20.PNG)